{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seminar 2: SLY(Sly Lex Yacc)\n",
    "## Lexer analyzer\n",
    "\n",
    "### Introduction\n",
    "\n",
    "SLY is library for writing parsers and compilers. It is loosely based on the traditional compiler construction tools lex and yacc and implements the same LALR(1) parsing algorithm. Most of the features available in lex and yacc are also available in SLY.\n",
    "\n",
    "SLY provides two separate classes Lexer and Parser. The Lexer class is used to break input text into a collection of tokens specified by a collection of regular expression rules. The Parser class is used to recognize language syntax that has been specified in the form of a context free grammar. The two classes are typically used together to make a parser. However, this is not a strict requirement–there is a great deal of flexibility allowed. The next two parts describe the basics.\n",
    "\n",
    "### Writing a Lexer\n",
    "\n",
    "Suppose you are writing a programming language and you wanted to parse the following input string:\n",
    "\n",
    "x = 3 + 42 * (s - t)\n",
    "\n",
    "The first step of parsing is to break the text into tokens where each token has a type and value. For example, the above text might be described by the following list of token tuples:\n",
    "\n",
    "[ ('ID','x'), ('EQUALS','='), ('NUMBER','3'),\n",
    "  ('PLUS','+'), ('NUMBER','42'), ('TIMES','*'),\n",
    "  ('LPAREN','('), ('ID','s'), ('MINUS','-'),\n",
    "  ('ID','t'), ('RPAREN',')') ]\n",
    "  \n",
    "The SLY Lexer class is used to do this. Here is a sample of a simple lexer that tokenizes the above text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type='ID', value='x'\n",
      "type='ASSIGN', value='='\n",
      "type='NUMBER', value='3'\n",
      "type='PLUS', value='+'\n",
      "type='NUMBER', value='42'\n",
      "type='TIMES', value='*'\n",
      "type='LPAREN', value='('\n",
      "type='ID', value='s2'\n",
      "type='MINUS', value='-'\n",
      "type='ID', value='tqw'\n",
      "type='RPAREN', value=')'\n"
     ]
    }
   ],
   "source": [
    "# calclex.py\n",
    "\n",
    "from sly import Lexer\n",
    "\n",
    "class CalcLexer(Lexer):\n",
    "    #Set of token names.   This is always required\n",
    "    tokens={ID, NUMBER, PLUS, MINUS, TIMES, DIVIDE, ASSIGN, LPAREN, RPAREN }\n",
    "\n",
    "    #String containing ignored characters between tokens\n",
    "    ignore = ' \\t'\n",
    "\n",
    "    #Regular expression rules for tokens\n",
    "    ID      = r'[a-zA-Z][a-zA-Z0-9]*'\n",
    "    NUMBER  = r'\\d+'\n",
    "    PLUS    = r'\\+'\n",
    "    MINUS   = r'-'\n",
    "    TIMES   = r'\\*'\n",
    "    DIVIDE  = r'/'\n",
    "    ASSIGN  = r'='\n",
    "    LPAREN  = r'\\('\n",
    "    RPAREN  = r'\\)'\n",
    "    #\\ is used to specify the caracter not a RE function.\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data = 'x = 3 + 42 * (s2 - tqw)'\n",
    "    lexer = CalcLexer()\n",
    "    for tok in lexer.tokenize(data):\n",
    "        print('type=%r, value=%r' % (tok.type, tok.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The tokens set\n",
    "Lexers must specify a tokens set that defines all of the possible token type names that can be produced by the lexer. This is always required and is used to perform a variety of validation checks.\n",
    "\n",
    "In the example, the following code specified the token names:\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CalcLexer(Lexer):\n",
    "    ...\n",
    "    # Set of token names.   This is always required\n",
    "    tokens = { ID, NUMBER, PLUS, MINUS, TIMES,\n",
    "               DIVIDE, ASSIGN, LPAREN, RPAREN }\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token names should be specified using all-caps as shown.\n",
    "\n",
    "### Specification of token match patterns\n",
    "Tokens are specified by writing a regular expression rule compatible with the re module. The name of each rule must match one of the names of the tokens provided in the tokens set. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLUS = r'\\+'\n",
    "MINUS = r'-'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expression patterns are compiled using the re.VERBOSE flag which can be used to help readability. However, unescaped whitespace is ignored and comments are allowed in this mode. If your pattern involves whitespace, make sure you use \\s. If you need to match the # character, use [#] or \\#.\n",
    "\n",
    "Tokens are matched in the same order that patterns are listed in the Lexer class. Longer tokens always need to be specified before short tokens. For example, if you wanted to have separate tokens for = and ==, you need to make sure that == is listed first. For example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**, LONGER TO SHORTER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLexer(Lexer):\n",
    "    tokens = { ASSIGN, EQ, ...}\n",
    "    ...\n",
    "    EQ     = r'=='       # MUST APPEAR FIRST! (LONGER)\n",
    "    ASSIGN = r'='"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discarded text\n",
    "The special ignore specification is reserved for single characters that should be completely ignored between tokens in the input stream. Usually this is used to skip over whitespace and other non-essential characters. The characters given in ignore are not ignored when such characters are part of other regular expression patterns. For example, if you had a rule to capture quoted text, that pattern can include the ignored characters (which will be captured in the normal way). The main purpose of ignore is to ignore whitespace and other padding between the tokens that you actually want to parse.\n",
    "\n",
    "You can also discard more specialized text patterns by writing special regular expression rules with a name that includes the prefix ignore_. For example, this lexer includes rules to ignore comments and newlines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type='ID', value='x'\n",
      "type='ASSIGN', value='='\n",
      "type='NUMBER', value=3\n",
      "type='PLUS', value='+'\n",
      "type='NUMBER', value=42\n",
      "type='TIMES', value='*'\n",
      "type='LPAREN', value='('\n",
      "type='ID', value='s'\n",
      "type='MINUS', value='-'\n",
      "type='ID', value='t'\n",
      "type='RPAREN', value=')'\n"
     ]
    }
   ],
   "source": [
    "# calclex.py\n",
    "\n",
    "from sly import Lexer\n",
    "\n",
    "class CalcLexer(Lexer):\n",
    "    tokens={ID, NUMBER, PLUS, MINUS, TIMES, DIVIDE, ASSIGN, LPAREN, RPAREN }\n",
    "    # String containing ignored characters (between tokens)\n",
    "    ignore = ' \\t'\n",
    "\n",
    "    # Other ignored patterns\n",
    "    ignore_comment = r'\\#.*'\n",
    "    ignore_newline = r'\\n+'\n",
    "    ID      = r'[a-zA-Z][a-zA-Z0-9]*'\n",
    "    NUMBER  = r'\\d+'\n",
    "    def NUMBER(self, t):\n",
    "        t.value = int(t.value)\n",
    "        return t\n",
    "    PLUS    = r'\\+'\n",
    "    MINUS   = r'-'\n",
    "    TIMES   = r'\\*'\n",
    "    DIVIDE  = r'/'\n",
    "    ASSIGN  = r'='\n",
    "    LPAREN  = r'\\('\n",
    "    RPAREN  = r'\\)'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data = '''x = 3 + 42\n",
    "                * (s    # This is a comment\n",
    "                    - t)'''\n",
    "    lexer = CalcLexer()\n",
    "    for tok in lexer.tokenize(data):\n",
    "        print('type=%r, value=%r' % (tok.type, tok.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Match Actions\n",
    "When certain tokens are matched, you may want to trigger some kind of action that performs extra processing. For example, converting a numeric value or looking up language keywords. One way to do this is to write your action as a method and give the associated regular expression using the @_() decorator like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-1426ac1b97c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m@\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'\\d+'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mNUMBER\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# Convert to a numeric value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "@_(r'\\d+') #Me resulta personalmente fea.\n",
    "def NUMBER(self, t):\n",
    "    t.value = int(t.value)   # Convert to a numeric value\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method always takes a single argument which is an instance of type Token. By default, t.type is set to the name of the token (e.g., 'NUMBER'). The function can change the token type and value as it sees appropriate. When finished, the resulting token object should be returned as a result. If no value is returned by the function, the token is discarded and the next token read.\n",
    "\n",
    "The @_() decorator is defined automatically within the Lexer class–you don’t need to do any kind of special import for it. It can also accept multiple regular expression rules. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@_(r'0x[0-9a-fA-F]+', #Me sigue sin gustar jeje\n",
    "   r'\\d+')\n",
    "def NUMBER(self, t):\n",
    "    if t.value.startswith('0x'):\n",
    "        t.value = int(t.value[2:], 16)\n",
    "    else:\n",
    "        t.value = int(t.value)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using the @_() decorator, you can also write a method that matches the same name as a token previously specified as a string. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER = r'\\d+'\n",
    "...\n",
    "def NUMBER(self, t):\n",
    "    t.value = int(t.value)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is potentially useful trick for debugging a lexer. You can temporarily attach a method a token and have it execute when the token is encountered. If you later take the method away, the lexer will revert back to its original behavior.\n",
    "\n",
    "### Token Remapping\n",
    "Occasionally, you might need to remap tokens based on special cases. Consider the case of matching identifiers such as “abc”, “python”, or “guido”. Certain identifiers such as “if”, “else”, and “while” might need to be treated as special keywords. To handle this, include token remapping rules when writing the lexer like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calclex.py\n",
    "\n",
    "from sly import Lexer\n",
    "\n",
    "class CalcLexer(Lexer):\n",
    "    tokens = { ID, IF, ELSE, WHILE }\n",
    "    # String containing ignored characters (between tokens)\n",
    "    ignore = ' \\t'\n",
    "\n",
    "    # Base ID rule\n",
    "    ID = r'[a-zA-Z_][a-zA-Z0-9_]*'\n",
    "\n",
    "    # Special cases\n",
    "    ID['if'] = IF\n",
    "    ID['else'] = ELSE\n",
    "    ID['while'] = WHILE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When parsing an identifier, the special cases will remap certain matching values to a new token type. For example, if the value of an identifier is “if” above, an IF token will be generated.\n",
    "\n",
    "### Line numbers and position tracking\n",
    "By default, lexers know nothing about line numbers. This is because they don’t know anything about what constitutes a “line” of input (e.g., the newline character or even if the input is textual data). To update this information, you need to add a special rule for newlines. Promote the ignore_newline rule to a method like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a rule so we can track line numbers\n",
    "@_(r'\\n+')\n",
    "def ignore_newline(self, t):\n",
    "    self.lineno += len(t.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the rule, the lineno attribute of the lexer is now updated. After the line number is updated, the token is discarded since nothing is returned.\n",
    "\n",
    "Lexers do not perform and kind of automatic column tracking. However, it does record positional information related to each token in the token’s index attribute. Using this, it is usually possible to compute column information as a separate step. For instance, you can search backwards until you reach the previous newline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute column.\n",
    "#     input is the input text string\n",
    "#     token is a token instance\n",
    "def find_column(text, token):\n",
    "    last_cr = text.rfind('\\n', 0, token.index)\n",
    "    if last_cr < 0:\n",
    "        last_cr = 0\n",
    "    column = (token.index - last_cr) + 1\n",
    "    return column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since column information is often only useful in the context of error handling, calculating the column position can be performed when needed as opposed to including it on each token.\n",
    "\n",
    "Literal characters\n",
    "Literal characters can be specified by defining a set literals in the class. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "LexerBuildError",
     "evalue": "MyLexer class does not define a tokens attribute",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLexerBuildError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-d7efafd56663>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mMyLexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mliterals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m \u001b[1;34m'+'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'-'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'*'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'/'\u001b[0m \u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sly\\lex.py\u001b[0m in \u001b[0;36m__new__\u001b[1;34m(meta, clsname, bases, attributes)\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_before\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattributes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbefore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delete\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattributes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m         \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sly\\lex.py\u001b[0m in \u001b[0;36m_build\u001b[1;34m(cls)\u001b[0m\n\u001b[0;32m    267\u001b[0m         '''\n\u001b[0;32m    268\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'tokens'\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvars\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mLexerBuildError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{cls.__qualname__} class does not define a tokens attribute'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m         \u001b[1;31m# Pull definitions created for any parent classes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLexerBuildError\u001b[0m: MyLexer class does not define a tokens attribute"
     ]
    }
   ],
   "source": [
    "class MyLexer(Lexer):\n",
    "    ...\n",
    "    literals = { '+','-','*','/' }\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A literal character is a single character that is returned “as is” when encountered by the lexer. Literals are checked after all of the defined regular expression rules. Thus, if a rule starts with one of the literal characters, it will always take precedence.\n",
    "\n",
    "When a literal token is returned, both its type and value attributes are set to the character itself. For example, '+'.\n",
    "\n",
    "It’s possible to write token methods that perform additional actions when literals are matched. However, you’ll need to set the token type appropriately. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLexer(Lexer):\n",
    "\n",
    "     literals = { '{', '}' }\n",
    "\n",
    "     def __init__(self):\n",
    "         self.nesting_level = 0\n",
    "\n",
    "     @_(r'\\{')\n",
    "     def lbrace(self, t):\n",
    "         t.type = '{'      # Set token type to the expected literal\n",
    "         self.nesting_level += 1\n",
    "         return t\n",
    "\n",
    "     @_(r'\\}')\n",
    "     def rbrace(t):\n",
    "         t.type = '}'      # Set token type to the expected literal\n",
    "         self.nesting_level -=1\n",
    "         return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error handling\n",
    "If a bad character is encountered while lexing, tokenizing will stop. However, you can add an error() method to handle lexing errors that occur when illegal characters are detected. The error method receives a Token where the value attribute contains all remaining untokenized text. A typical handler might look at this text and skip ahead in some manner. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLexer(Lexer):\n",
    "    ...\n",
    "    # Error handling rule\n",
    "    def error(self, t):\n",
    "        print(\"Illegal character '%s'\" % t.value[0])\n",
    "        self.index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we print the offending character and skip ahead one character by updating the lexer position. Error handling in a parser is often a hard problem. An error handler might scan ahead to a logical synchronization point such as a semicolon, a blank line, or similar landmark.\n",
    "\n",
    "If the error() method also returns the passed token, it will show up as an ERROR token in the resulting token stream. This might be useful if the parser wants to see error tokens for some reason–perhaps for the purposes of improved error messages or some other kind of error handling.\n",
    "\n",
    "### A More Complete Example\n",
    "Here is a more complete example that puts many of these concepts into practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token(type='ID', value='x', lineno=3, index=12)\n",
      "Token(type='ASSIGN', value='=', lineno=3, index=14)\n",
      "Token(type='NUMBER', value=0, lineno=3, index=16)\n",
      "Line 3: Bad character ';'\n",
      "Token(type='WHILE', value='while', lineno=4, index=19)\n",
      "Line 4: Bad character '('\n",
      "Token(type='ID', value='x', lineno=4, index=26)\n",
      "Token(type='LT', value='<', lineno=4, index=28)\n",
      "Token(type='NUMBER', value=10, lineno=4, index=30)\n",
      "Line 4: Bad character ')'\n",
      "Line 4: Bad character '{'\n",
      "Token(type='PRINT', value='print', lineno=5, index=40)\n",
      "Token(type='ID', value='x', lineno=5, index=46)\n",
      "Line 5: Bad character ':'\n",
      "Token(type='ID', value='x', lineno=6, index=53)\n",
      "Token(type='ASSIGN', value='=', lineno=6, index=55)\n",
      "Token(type='ID', value='x', lineno=6, index=57)\n",
      "Token(type='PLUS', value='+', lineno=6, index=59)\n",
      "Token(type='NUMBER', value=1, lineno=6, index=61)\n",
      "Line 6: Bad character ';'\n",
      "Line 7: Bad character '}'\n"
     ]
    }
   ],
   "source": [
    "# calclex.py\n",
    "\n",
    "from sly import Lexer\n",
    "\n",
    "class CalcLexer(Lexer):\n",
    "    # Set of token names.   This is always required\n",
    "    tokens = { NUMBER, ID, WHILE, IF, ELSE, PRINT,\n",
    "               PLUS, MINUS, TIMES, DIVIDE, ASSIGN,\n",
    "               EQ, LT, LE, GT, GE, NE }\n",
    "\n",
    "\n",
    "    literales = { '(', ')', '{', '}', ';' }\n",
    "\n",
    "    # String containing ignored characters\n",
    "    ignore = ' \\t'\n",
    "\n",
    "    # Regular expression rules for tokens\n",
    "    PLUS    = r'\\+'\n",
    "    MINUS   = r'-'\n",
    "    TIMES   = r'\\*'\n",
    "    DIVIDE  = r'/'\n",
    "    EQ      = r'=='\n",
    "    ASSIGN  = r'='\n",
    "    LE      = r'<='\n",
    "    LT      = r'<'\n",
    "    GE      = r'>='\n",
    "    GT      = r'>'   \n",
    "    NE      = r'!='\n",
    "\n",
    "    @_(r'\\d+')\n",
    "    def NUMBER(self, t):\n",
    "        t.value = int(t.value)\n",
    "        return t\n",
    "\n",
    "    # Identifiers and keywords\n",
    "    ID = r'[a-zA-Z_][a-zA-Z0-9_]*'\n",
    "    ID['if'] = IF\n",
    "    ID['else'] = ELSE\n",
    "    ID['while'] = WHILE\n",
    "    ID['print'] = PRINT\n",
    "\n",
    "    ignore_comment = r'\\#.*'\n",
    "\n",
    "    # Line number tracking\n",
    "    @_(r'\\n+')\n",
    "    def ignore_newline(self, t):\n",
    "        self.lineno += t.value.count('\\n')\n",
    "\n",
    "    def error(self, t):\n",
    "        print('Line %d: Bad character %r' % (self.lineno, t.value[0]))\n",
    "        self.index += 1\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data = '''\n",
    "# Counting\n",
    "x = 0;\n",
    "while (x < 10) {\n",
    "    print x:\n",
    "    x = x + 1;\n",
    "}\n",
    "'''\n",
    "    lexer = CalcLexer()\n",
    "    for tok in lexer.tokenize(data):\n",
    "        print(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study this example closely. It might take a bit to digest, but all of the essential parts of writing a lexer are there. Tokens have to be specified with regular expression rules. You can optionally attach actions that execute when certain patterns are encountered. Certain features such as character literals are there mainly for convenience, saving you the trouble of writing separate regular expression rules. You can also add error handling.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
